{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np, matplotlib.pyplot as plt\n",
    "folder = \"./final/\" # folder containing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0 class\n",
      "Loaded 1 class\n",
      "Loaded 2 class\n",
      "Loaded 3 class\n",
      "Loaded 4 class\n",
      "Loaded 5 class\n",
      "Loaded 6 class\n",
      "Loaded 7 class\n",
      "Loaded 8 class\n",
      "Loaded 9 class\n",
      "After reshaping\n",
      "(60000, 784) (60000, 10)\n",
      "[0.         0.         0.         0.         0.         0.00392157\n",
      " 0.         0.         0.         0.         0.15686275 0.7372549\n",
      " 0.40392157 0.21176471 0.1882353  0.16862746 0.34117648 0.65882355\n",
      " 0.52156866 0.0627451  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.00392157 0.         0.         0.         0.1882353\n",
      " 0.53333336 0.85882354 0.84705883 0.8901961  0.9254902  1.\n",
      " 1.         1.         1.         0.8509804  0.84313726 0.99607843\n",
      " 0.90588236 0.627451   0.17254902 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.05490196 0.6901961  0.87058824 0.8784314  0.827451\n",
      " 0.79607844 0.7764706  0.7647059  0.78431374 0.84313726 0.8\n",
      " 0.7921569  0.7882353  0.7882353  0.7882353  0.81960785 0.85490197\n",
      " 0.8784314  0.6392157  0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.7372549\n",
      " 0.85882354 0.78431374 0.7764706  0.7921569  0.7764706  0.78039217\n",
      " 0.78039217 0.7882353  0.7647059  0.7764706  0.7764706  0.78431374\n",
      " 0.78431374 0.78431374 0.78431374 0.7882353  0.78431374 0.88235295\n",
      " 0.15686275 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.2        0.85882354 0.78039217 0.79607844\n",
      " 0.79607844 0.827451   0.93333334 0.972549   0.98039216 0.9607843\n",
      " 0.9764706  0.9647059  0.96862745 0.9882353  0.972549   0.92156863\n",
      " 0.8117647  0.79607844 0.79607844 0.87058824 0.54901963 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.45490196 0.8862745  0.80784315 0.8        0.8117647  0.8\n",
      " 0.39607844 0.29411766 0.18431373 0.28627452 0.1882353  0.19607843\n",
      " 0.17254902 0.2        0.24705882 0.44313726 0.87058824 0.7921569\n",
      " 0.80784315 0.8627451  0.8784314  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.78431374 0.87058824\n",
      " 0.81960785 0.79607844 0.84313726 0.78431374 0.         0.27450982\n",
      " 0.38039216 0.         0.40392157 0.23137255 0.26666668 0.2784314\n",
      " 0.1882353  0.         0.85882354 0.80784315 0.8392157  0.8235294\n",
      " 0.98039216 0.14901961 0.         0.         0.         0.\n",
      " 0.         0.         0.96862745 0.85490197 0.827451   0.8235294\n",
      " 0.84313726 0.8392157  0.         0.99607843 0.9529412  0.54509807\n",
      " 1.         0.68235296 0.9843137  1.         0.8039216  0.\n",
      " 0.84313726 0.8509804  0.8392157  0.8156863  0.8627451  0.37254903\n",
      " 0.         0.         0.         0.         0.         0.17254902\n",
      " 0.8862745  0.8392157  0.8392157  0.84313726 0.8784314  0.8039216\n",
      " 0.         0.16470589 0.13725491 0.23529412 0.0627451  0.06666667\n",
      " 0.04705882 0.05098039 0.27450982 0.         0.7411765  0.84705883\n",
      " 0.827451   0.80784315 0.827451   0.6117647  0.         0.\n",
      " 0.         0.         0.         0.6392157  0.92156863 0.8392157\n",
      " 0.827451   0.8627451  0.84705883 0.7882353  0.20392157 0.2784314\n",
      " 0.34901962 0.36862746 0.3254902  0.30588236 0.27450982 0.29803923\n",
      " 0.36078432 0.34117648 0.80784315 0.8117647  0.87058824 0.8352941\n",
      " 0.85882354 0.8156863  0.         0.         0.         0.\n",
      " 0.         0.4117647  0.73333335 0.8745098  0.92941177 0.972549\n",
      " 0.827451   0.7764706  0.9882353  0.98039216 0.972549   0.9607843\n",
      " 0.972549   0.9882353  0.99215686 0.98039216 0.9882353  0.9372549\n",
      " 0.7882353  0.827451   0.88235295 0.84313726 0.75686276 0.44313726\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.06666667 0.21176471 0.62352943 0.87058824 0.75686276\n",
      " 0.8156863  0.7529412  0.77254903 0.78431374 0.78431374 0.78431374\n",
      " 0.78431374 0.7882353  0.79607844 0.7647059  0.8235294  0.64705884\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.18431373 0.88235295 0.7529412  0.8392157  0.79607844\n",
      " 0.80784315 0.8        0.8        0.8039216  0.80784315 0.8\n",
      " 0.827451   0.77254903 0.85490197 0.41960785 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.00392157 0.02352941 0.         0.18039216\n",
      " 0.827451   0.7647059  0.827451   0.7921569  0.80784315 0.8039216\n",
      " 0.8        0.8039216  0.80784315 0.8        0.827451   0.78431374\n",
      " 0.85490197 0.35686275 0.         0.01176471 0.00392157 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.00392157 0.         0.04313726 0.77254903 0.78039217\n",
      " 0.8039216  0.7921569  0.8039216  0.80784315 0.8        0.8039216\n",
      " 0.8117647  0.8        0.8039216  0.8039216  0.85490197 0.3019608\n",
      " 0.         0.01960784 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.01176471\n",
      " 0.         0.00784314 0.7490196  0.7764706  0.7882353  0.8039216\n",
      " 0.80784315 0.8039216  0.8039216  0.80784315 0.81960785 0.80784315\n",
      " 0.78039217 0.81960785 0.85882354 0.28627452 0.         0.01960784\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.00784314 0.         0.\n",
      " 0.7372549  0.77254903 0.78431374 0.8117647  0.8117647  0.8\n",
      " 0.8117647  0.8117647  0.8235294  0.8156863  0.7764706  0.8117647\n",
      " 0.8666667  0.28235295 0.         0.01568628 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.00784314 0.         0.         0.84313726 0.7764706\n",
      " 0.79607844 0.80784315 0.8156863  0.8039216  0.8117647  0.8117647\n",
      " 0.8235294  0.8156863  0.78431374 0.7921569  0.87058824 0.29411766\n",
      " 0.         0.01568628 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.00392157\n",
      " 0.         0.         0.827451   0.7764706  0.81960785 0.80784315\n",
      " 0.81960785 0.80784315 0.8156863  0.8117647  0.827451   0.80784315\n",
      " 0.8039216  0.7764706  0.8666667  0.3137255  0.         0.01176471\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.00392157 0.         0.\n",
      " 0.8        0.7882353  0.8039216  0.8156863  0.8117647  0.8039216\n",
      " 0.827451   0.8039216  0.8235294  0.8235294  0.81960785 0.7647059\n",
      " 0.8666667  0.3764706  0.         0.01176471 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.00392157 0.         0.         0.7921569  0.7882353\n",
      " 0.8039216  0.81960785 0.8117647  0.8039216  0.8352941  0.80784315\n",
      " 0.8235294  0.81960785 0.8235294  0.7607843  0.8509804  0.4117647\n",
      " 0.         0.00784314 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.00392157\n",
      " 0.         0.         0.8        0.8        0.8039216  0.8156863\n",
      " 0.8117647  0.8039216  0.84313726 0.8117647  0.8235294  0.8156863\n",
      " 0.827451   0.75686276 0.8352941  0.4509804  0.         0.00784314\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.8        0.8117647  0.8117647  0.8156863  0.80784315 0.80784315\n",
      " 0.84313726 0.8235294  0.8235294  0.8117647  0.827451   0.7647059\n",
      " 0.8235294  0.4627451  0.         0.00784314 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.00392157 0.         0.         0.7764706  0.8156863\n",
      " 0.8156863  0.8156863  0.8        0.8117647  0.827451   0.827451\n",
      " 0.8235294  0.8117647  0.827451   0.7647059  0.8117647  0.4745098\n",
      " 0.         0.00392157 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.00392157\n",
      " 0.         0.         0.7764706  0.8235294  0.8117647  0.8156863\n",
      " 0.80784315 0.81960785 0.8352941  0.827451   0.827451   0.8117647\n",
      " 0.8235294  0.77254903 0.8117647  0.4862745  0.         0.00392157\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.6745098  0.8235294  0.79607844 0.7882353  0.78039217 0.8\n",
      " 0.8117647  0.8039216  0.8        0.7882353  0.8039216  0.77254903\n",
      " 0.80784315 0.49803922 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.7372549  0.8666667\n",
      " 0.8392157  0.91764706 0.9254902  0.93333334 0.9529412  0.9529412\n",
      " 0.9529412  0.9411765  0.9529412  0.8392157  0.8784314  0.63529414\n",
      " 0.         0.00784314 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.00392157\n",
      " 0.         0.         0.54509807 0.57254905 0.50980395 0.5294118\n",
      " 0.5294118  0.5372549  0.49019608 0.4862745  0.49019608 0.4745098\n",
      " 0.46666667 0.44313726 0.50980395 0.29803923 0.         0.\n",
      " 0.         0.         0.         0.        ] [1 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Now we try to load the images and their corresponding labels into memory\n",
    "\n",
    "def load_data(X, y):\n",
    "    for f in os.listdir(folder):\n",
    "        for file in os.listdir(f\"{folder}/{f}\"):\n",
    "            img = plt.imread(f\"{folder}/{f}/{file}\")\n",
    "            X.append(img)\n",
    "\n",
    "            # The most obvious choice for the label is the class (folder) name\n",
    "            # label = int(f)\n",
    "\n",
    "            # [Q1] But we dont use it here why? Why is it an array of 10 elements?\n",
    "            # Clue: Lookup one hot encoding\n",
    "            # Read up on Cross Entropy Loss\n",
    "\n",
    "            label = [0] * 10\n",
    "            label[int(f)] = 1 # Why is this array the label and not a numeber?\n",
    "\n",
    "            y.append(label)\n",
    "            \n",
    "        print(f\"Loaded {f} class\")\n",
    "\n",
    "X, y = [], []\n",
    "load_data(X, y)\n",
    "\n",
    "# [Q2] Why convert to numpy array?\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "X = X[:, :,:, 0] # [Q3] Why are we doing this and what does this type of slicing result in?\n",
    "X = X.reshape(X.shape[0], X.shape[1]*X.shape[2]) # [Q4] Why are we reshaping the data?\n",
    "print(\"After reshaping\")\n",
    "print(X.shape, y.shape)\n",
    "print(X[0], y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, input_neurons, hidden_neurons, output_neurons, learning_rate, epochs):\n",
    "        \"\"\"\n",
    "        Class Definition\n",
    "        \n",
    "        We use a class because it is easy to visualize the process of training a neural network\n",
    "        It's also easier to resuse and repurpose depending on the task at hand\n",
    "\n",
    "        We have a simple neural network, with an input layer, one hidden (middle) layer and an output layer\n",
    "\n",
    "        input_neurons: Number of neurons in the input layer\n",
    "        hidden_neurons: Number of neurons in the hidden layer\n",
    "        output_neurons: Number of neurons in the output layer\n",
    "        learning_rate: The rate at which the weights are updated [Q5] What is the learning rate?\n",
    "        epochs: Number of times the model will train on the entire dataset \n",
    "        \"\"\"\n",
    "\n",
    "        self.input_neurons = input_neurons\n",
    "        self.hidden_neurons = hidden_neurons\n",
    "        self.output_neurons = output_neurons\n",
    "        self.epochs = epochs\n",
    "\n",
    "        self.lr = learning_rate\n",
    "\n",
    "        \"\"\"\n",
    "        Weights and Biases\n",
    "\n",
    "        At this point you should know what weights and biases are in a neural network and if not, go check out the 3blue1brown video on Neural Networks\n",
    "        What matters here is however the matrix dimensions of the weights and biases\n",
    "\n",
    "        [Q6] Why are the dimensions of the weights and biases the way they are?\n",
    "        \n",
    "        Try to figure out the dimensions of the weights and biases for the hidden and output layers\n",
    "        Try to see what equations represent the forward pass (basically the prediction)\n",
    "        And then, try to see if the dimensions of the matrix multiplications are correct\n",
    "\n",
    "        Note: The bias dimensions may not match. Look up broadcasting in numpy to understand\n",
    "        [Q7] What is broadcasting and why do we need to broadcast the bias?\n",
    "        \"\"\"\n",
    "\n",
    "        # Ideally any random set of weights and biases can be used to initialize the network\n",
    "        # self.wih = np.random.randn(hidden_neurons, input_neurons)\n",
    "\n",
    "        # [Q8] What is np.random.randn? What's the shape of this matrix?\n",
    "\n",
    "        # Optional: Try to figure out why the weights are initialized this way\n",
    "        # Note: You can just use the commented out line above if you don't want to do this\n",
    "\n",
    "        self.wih = np.random.randn(hidden_neurons, input_neurons) * np.sqrt(2/input_neurons)\n",
    "        self.bih = np.zeros((hidden_neurons, 1))\n",
    "\n",
    "        self.who = np.random.randn(output_neurons, hidden_neurons) * np.sqrt(2/hidden_neurons)\n",
    "        self.bho = np.zeros((output_neurons, 1))\n",
    "\n",
    "    # Activation Functions and their derivatives\n",
    "    # [Q9] What are activation functions and why do we need them?\n",
    "\n",
    "    def relu(self, z):\n",
    "        \"\"\"\n",
    "        Implementation of the RELU function\n",
    "        z: (n, 1)\n",
    "        returns (n, 1)\n",
    "        \"\"\"\n",
    "        return z * (z > 0)\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Implementation of the Sigmoid function\n",
    "        z: (n, 1)\n",
    "        returns (n, 1)\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def relu_derivative(self, z):\n",
    "        \"\"\"\n",
    "        Implementation of the RELU derivative function\n",
    "        z: (n, 1)\n",
    "        returns (n, 1)\n",
    "        \"\"\"\n",
    "        return 1 * (z > 0)\n",
    "\n",
    "    def sigmoid_derivative(self, z):\n",
    "        \"\"\"\n",
    "        Implementation of the Sigmoid derivative function\n",
    "        z: (n, 1)\n",
    "        returns (n, 1)\n",
    "        \"\"\"\n",
    "        return z * (1 - z)\n",
    "\n",
    "    # [Q10] What is the softmax function and why do we need it? Read up on it\n",
    "    def softmax(self, z):\n",
    "        \"\"\"\n",
    "        Implementation of the Softmax function\n",
    "        z: (n, 1)\n",
    "        returns (n, 1)\n",
    "        \"\"\"\n",
    "        return np.exp(z) / np.sum(np.exp(z), axis=0)\n",
    "\n",
    "    def softmax_derivative(self, z):\n",
    "        \"\"\"\n",
    "        Implementation of the Softmax derivative function\n",
    "        z: (n, 1)\n",
    "        returns (n, 1)\n",
    "        \"\"\"\n",
    "        return z * (1 - z)\n",
    "\n",
    "    # Loss Functions and their derivatives\n",
    "    # [Q11] What are loss functions and why do we need them?\n",
    "\n",
    "    def mean_squared_error(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        Implementation of the Mean Squared Error function\n",
    "        y: (10, n)\n",
    "        y_hat: (10, n)\n",
    "        returns (1, n)\n",
    "        \"\"\"\n",
    "        return np.mean((y - y_hat) ** 2, axis=0)\n",
    "\n",
    "    def cross_entropy_loss(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        Implementation of the Cross Entropy Loss function\n",
    "        y: (10, n)\n",
    "        y_hat: (10, n)\n",
    "        returns (1, n)\n",
    "        \"\"\"\n",
    "\n",
    "        # Implement the cross entropy loss function here and return it\n",
    "        # Keep the dimensions of the input in mind when writing the code\n",
    "\n",
    "        epsilon = 1e-9\n",
    "        loss = -np.sum(y * np.log(y_hat + epsilon), axis=0)\n",
    "        return loss \n",
    "\n",
    "    def mean_squared_error_derivative(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        Implementation of the Mean Squared Error derivative function\n",
    "        y: (10, n)\n",
    "        y_hat: (10, n)\n",
    "        returns (10, n)\n",
    "        \"\"\"\n",
    "        return y_hat - y\n",
    "\n",
    "    def cross_entropy_derivative(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        Implementation of the Cross Entropy Loss derivative function\n",
    "        y: (10, n)\n",
    "        y_hat: (10, n)\n",
    "        returns (10, n)\n",
    "        \"\"\"\n",
    "\n",
    "        # Implement the cross entropy loss derivative function here and return it\n",
    "        # Note: The derivative of the CEL is usually taken with respect to the softmax input not output so keep that in mind while writing\n",
    "\n",
    "        return y_hat - y\n",
    "\n",
    "    # Forward propagation\n",
    "    def forward(self, input_list):\n",
    "        \"\"\"\n",
    "        Implementation of the Forward Pass\n",
    "        input_list: (784, n)        - n is the number of images\n",
    "        returns (10, n)              - n is the number of images\n",
    "    \n",
    "        Now we come to the heart of the neural network, the forward pass\n",
    "        This is where the input is passed through the network to get the output\n",
    "\n",
    "        [Q12] What does the output choice we have here mean? It's an array of 10 elements per image, but why?\n",
    "        \"\"\"\n",
    "\n",
    "        inputs = np.array(input_list, ndmin=2).T\n",
    "        inputs = inputs - np.mean(inputs) # [Q13] Why are we subtracting the mean of the inputs?\n",
    "        \n",
    "        # To get to the hidden layer:\n",
    "        # Multiply the input with the weights and adding the bias\n",
    "        # Apply the activation function (relu in this case)\n",
    "\n",
    "        hidden_inputs = np.dot(self.wih, inputs) + self.bih\n",
    "        hidden_outputs = self.relu(hidden_inputs)\n",
    "\n",
    "        # To get to the output layer:\n",
    "        # Multiply the hidden layer output with the weights and adding the bias\n",
    "        # Apply the activation function (softmax in this case)\n",
    "        # [Q14] Why are we using the softmax function here?\n",
    "\n",
    "        final_inputs = np.dot(self.who, hidden_outputs) + self.bho\n",
    "        final_outputs = self.softmax(final_inputs)\n",
    "\n",
    "        # Return it\n",
    "\n",
    "        return final_outputs\n",
    "        \n",
    "\n",
    "    # Back propagation\n",
    "    def backprop(self, inputs_list, targets_list):\n",
    "        \"\"\"\n",
    "        Implementation of the Backward Pass\n",
    "        inputs_list: (784, n)\n",
    "        targets_list: (10, n)\n",
    "        returns a scalar value (loss)\n",
    "        \n",
    "        This is where the magic happens, the backpropagation algorithm\n",
    "        This is where the weights are updated based on the error in the prediction of the network\n",
    "\n",
    "        Now, the calculus involved is fairly complicated, especially because it's being done in matrix form\n",
    "        However the intuition is simple. \n",
    "\n",
    "        Since this is a recruitment stage, most of the function is written out for you, so follow along with the comments\n",
    "        \"\"\"\n",
    "\n",
    "        # Basic forward pass to get the outputs\n",
    "        # Obviously we need the predictions to know how the model is doing\n",
    "        # [Q15] Why are we doing a forward pass here instead of just using the outputs from the forward function?\n",
    "        # Is there any actual reason, or could we just swap it?\n",
    "\n",
    "        inputs = np.array(inputs_list, ndmin=2).T # (784, n)\n",
    "        inputs = inputs - np.mean(inputs)\n",
    "\n",
    "        tj = np.array(targets_list, ndmin=2).T # (10, n)\n",
    "\n",
    "        hidden_inputs = np.dot(self.wih, inputs) + self.bih\n",
    "        hidden_outputs = self.relu(hidden_inputs)\n",
    "\n",
    "        final_inputs = np.dot(self.who, hidden_outputs) + self.bho\n",
    "        yj = self.softmax(final_inputs)\n",
    "\n",
    "        # Calculating the loss - This is the error in the prediction\n",
    "        # The loss then is the indication of how well the model is doing, its a useful parameter to track to see if the model is improving\n",
    "\n",
    "        loss = self.mean_squared_error(tj, yj) # Convert this to cross entropy loss\n",
    "\n",
    "\n",
    "        # Updating the weights using Update Rule\n",
    "        # Now that we have the incorrect predictions, we can update the weights to make the predictions better\n",
    "        # This is done using the gradient of the loss function with respect to the weights\n",
    "        # Basically, we know how much the overall error is caused due to individual weights using the chain rule of calculus\n",
    "        # Since we want to minimise the error, we move in the opposite direction of something like a \"derivative\" of the error with respect to the weights\n",
    "        # Calculus therefore helps us find the direction in which we should move to reduce the error\n",
    "        # A direction means what delta W changes we need to make to make the model better\n",
    "\n",
    "        # Output Layer - We start with the output layer because we are backtracking how the error is caused\n",
    "        # Think of it as using the derivatives of each layer while going back\n",
    "\n",
    "\n",
    "        # For the task, you will be using Cross Entropy Loss\n",
    "\n",
    "        # Change this to cross entropy loss\n",
    "        dE_dzo = self.cross_entropy_derivative(tj, yj) # (10,n)\n",
    "        # Note: the derivative of the CEL is usually taken with respect to the softmax input not output so keep that in mind while writing\n",
    "\n",
    "\n",
    "        dE_dwho = np.dot(dE_dzo, hidden_outputs.T) / hidden_outputs.shape[1] # dot((10,n) (n,128) = (10,128)\n",
    "        dE_dbho = np.mean(dE_dzo, axis=1, keepdims=True) # sum((10,n), axis=1) = (10,1)\n",
    "        \n",
    "        self.who -= self.lr * dE_dwho\n",
    "        self.bho -= self.lr * dE_dbho\n",
    "\n",
    "        # Hidden Layer\n",
    "        dE_dah = np.dot(self.who.T, dE_dzo) # dot((128,10), (10,n)) = (128,n)\n",
    "        dE_dzh = dE_dah * self.relu_derivative(hidden_inputs)\n",
    "        dE_dwih = np.dot(dE_dzh, inputs.T) / inputs.shape[1]\n",
    "        dE_dbih = np.mean(dE_dzh, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "        self.wih -= self.lr * dE_dwih\n",
    "        self.bih -= self.lr * dE_dbih\n",
    "\n",
    "        return np.mean(loss)\n",
    "\n",
    "    def fit(self, inputs_list, targets_list,validation_data, validation_labels):\n",
    "        \"\"\"\n",
    "        Implementation of the training loop\n",
    "        inputs_list: (784, n)\n",
    "        targets_list: (10, n)\n",
    "        validation_data: (784, n)\n",
    "        validation_labels: (10, n)\n",
    "        returns train_loss, val_loss\n",
    "\n",
    "        This is where the training loop is implemented\n",
    "        We loop over the entire dataset for a certain number of epochs\n",
    "        We also track the validation loss to see how well the model is generalizing\n",
    "        [Q16] What is the validation dataset and what do we mean by generalization?\n",
    "\n",
    "        We also return the training and validation loss to see how the model is improving\n",
    "        It's a good idea to plot these to see how the model is doing\n",
    "        \"\"\"\n",
    "\n",
    "        train_loss = []\n",
    "        val_loss = []\n",
    "        for epoch in range(self.epochs):\n",
    "            loss = self.backprop(inputs_list, targets_list)\n",
    "            train_loss.append(loss)\n",
    "            vloss = self.mean_squared_error(validation_labels.T, self.forward(validation_data))\n",
    "            val_loss.append(np.mean(vloss)) \n",
    "            print(f\"Epoch: {epoch}, Loss: {loss}, Val Loss: {val_loss[-1]}\")\n",
    "\n",
    "        return train_loss[1:], val_loss[:-1] \n",
    "\n",
    "    def predict(self, X):\n",
    "        outputs = self.forward(X).T\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 42000\n",
      "Validation samples: 12000\n",
      "Test samples: 6000\n",
      "\n",
      "Training model...\n",
      "Epoch: 0, Loss: 0.0916629010392942, Val Loss: 0.08728205956066529\n",
      "Epoch: 1, Loss: 0.08728277758115867, Val Loss: 0.0832343433659407\n",
      "Epoch: 2, Loss: 0.08325571136235961, Val Loss: 0.07932684384024165\n",
      "Epoch: 3, Loss: 0.0793837142365057, Val Loss: 0.07553896716158626\n",
      "Epoch: 4, Loss: 0.07564170849315797, Val Loss: 0.07191169281110152\n",
      "Epoch: 5, Loss: 0.07206140228239816, Val Loss: 0.0685150648500781\n",
      "Epoch: 6, Loss: 0.06870603210840386, Val Loss: 0.06539601520064216\n",
      "Epoch: 7, Loss: 0.06562523040319355, Val Loss: 0.06257599342663286\n",
      "Epoch: 8, Loss: 0.0628388348444516, Val Loss: 0.06003863799650598\n",
      "Epoch: 9, Loss: 0.06033286168260299, Val Loss: 0.05776128195652357\n",
      "Epoch: 10, Loss: 0.0580857070659994, Val Loss: 0.05572429880472132\n",
      "Epoch: 11, Loss: 0.05607501343886259, Val Loss: 0.05389906876916647\n",
      "Epoch: 12, Loss: 0.05427659181403155, Val Loss: 0.052264145023055646\n",
      "Epoch: 13, Loss: 0.05266839232355712, Val Loss: 0.050797949680197435\n",
      "Epoch: 14, Loss: 0.05122657394019197, Val Loss: 0.04948185700950804\n",
      "Epoch: 15, Loss: 0.0499331003243534, Val Loss: 0.048297837160342375\n",
      "Epoch: 16, Loss: 0.048770401596853086, Val Loss: 0.04722820336860278\n",
      "Epoch: 17, Loss: 0.04772156185627797, Val Loss: 0.046259510302133135\n",
      "Epoch: 18, Loss: 0.04677228269302065, Val Loss: 0.04537880025920416\n",
      "Epoch: 19, Loss: 0.04591022979006724, Val Loss: 0.04457511150530712\n",
      "Epoch: 20, Loss: 0.04512391795769143, Val Loss: 0.043839867030321764\n",
      "Epoch: 21, Loss: 0.04440460974185095, Val Loss: 0.04316395408495102\n",
      "Epoch: 22, Loss: 0.04374397181823622, Val Loss: 0.042540916938778144\n",
      "Epoch: 23, Loss: 0.043135221700801224, Val Loss: 0.041964003491153035\n",
      "Epoch: 24, Loss: 0.04257195148597832, Val Loss: 0.041428675868553436\n",
      "Epoch: 25, Loss: 0.042049324301157875, Val Loss: 0.04093029953931404\n",
      "Epoch: 26, Loss: 0.04156292375577532, Val Loss: 0.040464849833342614\n",
      "Epoch: 27, Loss: 0.04110841183866602, Val Loss: 0.04002900708661001\n",
      "Epoch: 28, Loss: 0.040682779981860345, Val Loss: 0.039619951778730345\n",
      "Epoch: 29, Loss: 0.04028293412881284, Val Loss: 0.03923451008080792\n",
      "Epoch: 30, Loss: 0.03990608380559685, Val Loss: 0.038870907435833914\n",
      "Epoch: 31, Loss: 0.039550234300686354, Val Loss: 0.03852680863285816\n",
      "Epoch: 32, Loss: 0.03921344940674417, Val Loss: 0.03820061891010761\n",
      "Epoch: 33, Loss: 0.03889379990562288, Val Loss: 0.03789044028039503\n",
      "Epoch: 34, Loss: 0.038589586265128165, Val Loss: 0.03759504266587957\n",
      "Epoch: 35, Loss: 0.038299870202771154, Val Loss: 0.037313464451546145\n",
      "Epoch: 36, Loss: 0.03802355137365043, Val Loss: 0.0370440341482991\n",
      "Epoch: 37, Loss: 0.03775899124762821, Val Loss: 0.03678617400105025\n",
      "Epoch: 38, Loss: 0.037505622732733404, Val Loss: 0.03653900723579127\n",
      "Epoch: 39, Loss: 0.03726260734027062, Val Loss: 0.036301772871283625\n",
      "Epoch: 40, Loss: 0.03702913276818143, Val Loss: 0.03607369985021362\n",
      "Epoch: 41, Loss: 0.03680446033132712, Val Loss: 0.0358543223043167\n",
      "Epoch: 42, Loss: 0.03658804394891494, Val Loss: 0.0356427387863612\n",
      "Epoch: 43, Loss: 0.03637925989919973, Val Loss: 0.03543854347309836\n",
      "Epoch: 44, Loss: 0.03617781548059438, Val Loss: 0.03524119809894506\n",
      "Epoch: 45, Loss: 0.03598317124367897, Val Loss: 0.03505073137106054\n",
      "Epoch: 46, Loss: 0.03579494467715961, Val Loss: 0.034866433116471696\n",
      "Epoch: 47, Loss: 0.03561273893708545, Val Loss: 0.03468790354348481\n",
      "Epoch: 48, Loss: 0.03543622577983796, Val Loss: 0.03451485642024852\n",
      "Epoch: 49, Loss: 0.03526500510716436, Val Loss: 0.034346994914339776\n",
      "Epoch: 50, Loss: 0.03509872831674352, Val Loss: 0.034184138274915905\n",
      "Epoch: 51, Loss: 0.03493717480930071, Val Loss: 0.03402599367075095\n",
      "Epoch: 52, Loss: 0.03478012588358334, Val Loss: 0.03387236865232944\n",
      "Epoch: 53, Loss: 0.03462743662130885, Val Loss: 0.033722872375032104\n",
      "Epoch: 54, Loss: 0.03447872147054725, Val Loss: 0.03357745020241077\n",
      "Epoch: 55, Loss: 0.03433377223496255, Val Loss: 0.033435812940267644\n",
      "Epoch: 56, Loss: 0.03419247767433378, Val Loss: 0.03329755684957794\n",
      "Epoch: 57, Loss: 0.03405466867175017, Val Loss: 0.033162665733612054\n",
      "Epoch: 58, Loss: 0.03392012987625167, Val Loss: 0.03303101483830632\n",
      "Epoch: 59, Loss: 0.033788744471940184, Val Loss: 0.0329024842695894\n",
      "Epoch: 60, Loss: 0.03366039604676589, Val Loss: 0.032777075669380776\n",
      "Epoch: 61, Loss: 0.03353500520487512, Val Loss: 0.03265445469239953\n",
      "Epoch: 62, Loss: 0.033412394446952376, Val Loss: 0.032534560654027815\n",
      "Epoch: 63, Loss: 0.03329242090576151, Val Loss: 0.03241722339812409\n",
      "Epoch: 64, Loss: 0.033174936844590865, Val Loss: 0.03230237682469109\n",
      "Epoch: 65, Loss: 0.03305990735110493, Val Loss: 0.03218969457378042\n",
      "Epoch: 66, Loss: 0.03294704180839129, Val Loss: 0.03207941564666292\n",
      "Epoch: 67, Loss: 0.0328365339209255, Val Loss: 0.03197141577601956\n",
      "Epoch: 68, Loss: 0.03272817452969858, Val Loss: 0.03186552783812689\n",
      "Epoch: 69, Loss: 0.03262185453794655, Val Loss: 0.03176170050788164\n",
      "Epoch: 70, Loss: 0.03251757599002668, Val Loss: 0.0316599127903929\n",
      "Epoch: 71, Loss: 0.03241533007627794, Val Loss: 0.03156002975527775\n",
      "Epoch: 72, Loss: 0.03231487615517782, Val Loss: 0.031462115982330216\n",
      "Epoch: 73, Loss: 0.032216303404485384, Val Loss: 0.03136596640341265\n",
      "Epoch: 74, Loss: 0.032119546138269714, Val Loss: 0.031271737026084545\n",
      "Epoch: 75, Loss: 0.03202466389570001, Val Loss: 0.031179035615468004\n",
      "Epoch: 76, Loss: 0.0319313878365299, Val Loss: 0.031088002781556757\n",
      "Epoch: 77, Loss: 0.03183976954797259, Val Loss: 0.030998383134723476\n",
      "Epoch: 78, Loss: 0.03174968624245589, Val Loss: 0.030910731865134853\n",
      "Epoch: 79, Loss: 0.031661294931297296, Val Loss: 0.030824199792842198\n",
      "Epoch: 80, Loss: 0.031574152913028966, Val Loss: 0.030739318210649545\n",
      "Epoch: 81, Loss: 0.0314885180323795, Val Loss: 0.030655623363308186\n",
      "Epoch: 82, Loss: 0.031404154528245305, Val Loss: 0.03057349068701786\n",
      "Epoch: 83, Loss: 0.0313212409496271, Val Loss: 0.03049274101738544\n",
      "Epoch: 84, Loss: 0.031239724647807093, Val Loss: 0.030413241993386162\n",
      "Epoch: 85, Loss: 0.03115947561025001, Val Loss: 0.03033505603518043\n",
      "Epoch: 86, Loss: 0.03108046235274994, Val Loss: 0.030258056585023223\n",
      "Epoch: 87, Loss: 0.031002611190094252, Val Loss: 0.030182312565970353\n",
      "Epoch: 88, Loss: 0.03092597237130065, Val Loss: 0.030107861949193775\n",
      "Epoch: 89, Loss: 0.030850584633115578, Val Loss: 0.030034465537944005\n",
      "Epoch: 90, Loss: 0.03077628902999412, Val Loss: 0.029962236848778804\n",
      "Epoch: 91, Loss: 0.030703091322143813, Val Loss: 0.029891026725565387\n",
      "Epoch: 92, Loss: 0.030630894389142994, Val Loss: 0.029820915297367565\n",
      "Epoch: 93, Loss: 0.030559782632866517, Val Loss: 0.02975179690460344\n",
      "Epoch: 94, Loss: 0.030489701716833507, Val Loss: 0.029683720922833258\n",
      "Epoch: 95, Loss: 0.030420610228753222, Val Loss: 0.02961671705617581\n",
      "Epoch: 96, Loss: 0.030352496171392768, Val Loss: 0.029550605586418064\n",
      "Epoch: 97, Loss: 0.030285220140227716, Val Loss: 0.029485449987405127\n",
      "Epoch: 98, Loss: 0.030218840215926333, Val Loss: 0.029421323131143238\n",
      "Epoch: 99, Loss: 0.030153413088795558, Val Loss: 0.02935806565663694\n",
      "Epoch: 100, Loss: 0.03008889339580756, Val Loss: 0.029295775039747687\n",
      "Epoch: 101, Loss: 0.03002522574798353, Val Loss: 0.02923437221330329\n",
      "Epoch: 102, Loss: 0.029962408672011692, Val Loss: 0.02917383837021038\n",
      "Epoch: 103, Loss: 0.0299004487301441, Val Loss: 0.029114042871480953\n",
      "Epoch: 104, Loss: 0.029839267177719498, Val Loss: 0.029055033910626842\n",
      "Epoch: 105, Loss: 0.029778898383597114, Val Loss: 0.02899679219037143\n",
      "Epoch: 106, Loss: 0.029719305774315422, Val Loss: 0.028939264451529265\n",
      "Epoch: 107, Loss: 0.029660439589226543, Val Loss: 0.028882514700061732\n",
      "Epoch: 108, Loss: 0.029602287149829917, Val Loss: 0.028826458835132768\n",
      "Epoch: 109, Loss: 0.029544835886378906, Val Loss: 0.028771171637687134\n",
      "Epoch: 110, Loss: 0.029488104996053845, Val Loss: 0.028716485496895157\n",
      "Epoch: 111, Loss: 0.029432088872128514, Val Loss: 0.02866250092623306\n",
      "Epoch: 112, Loss: 0.029376805194292657, Val Loss: 0.028609118088327865\n",
      "Epoch: 113, Loss: 0.02932213873918306, Val Loss: 0.028556366435290196\n",
      "Epoch: 114, Loss: 0.02926813276269173, Val Loss: 0.0285042933227675\n",
      "Epoch: 115, Loss: 0.029214792511648085, Val Loss: 0.028452751248127007\n",
      "Epoch: 116, Loss: 0.029162052060729745, Val Loss: 0.02840186033298615\n",
      "Epoch: 117, Loss: 0.029109961188798897, Val Loss: 0.028351475923078982\n",
      "Epoch: 118, Loss: 0.029058445825474146, Val Loss: 0.028301729462643473\n",
      "Epoch: 119, Loss: 0.029007539804361056, Val Loss: 0.02825256998577198\n",
      "Epoch: 120, Loss: 0.02895721290570129, Val Loss: 0.02820398278757941\n",
      "Epoch: 121, Loss: 0.028907455817277473, Val Loss: 0.02815594290856489\n",
      "Epoch: 122, Loss: 0.02885827361982514, Val Loss: 0.02810853527583635\n",
      "Epoch: 123, Loss: 0.028809717894265847, Val Loss: 0.028061632709508493\n",
      "Epoch: 124, Loss: 0.028761706170590557, Val Loss: 0.028015226095389457\n",
      "Epoch: 125, Loss: 0.028714212636587976, Val Loss: 0.027969364365369224\n",
      "Epoch: 126, Loss: 0.028667236593207895, Val Loss: 0.027923878129589644\n",
      "Epoch: 127, Loss: 0.0286207374756201, Val Loss: 0.027879023258447502\n",
      "Epoch: 128, Loss: 0.028574765032559, Val Loss: 0.027834590484930147\n",
      "Epoch: 129, Loss: 0.028529285275052708, Val Loss: 0.027790704102411298\n",
      "Epoch: 130, Loss: 0.028484304580784804, Val Loss: 0.027747263521636804\n",
      "Epoch: 131, Loss: 0.028439774785280612, Val Loss: 0.027704325866214692\n",
      "Epoch: 132, Loss: 0.028395735776855936, Val Loss: 0.027661836122766743\n",
      "Epoch: 133, Loss: 0.028352175343802657, Val Loss: 0.027619816165181117\n",
      "Epoch: 134, Loss: 0.02830906265625743, Val Loss: 0.027578213439172168\n",
      "Epoch: 135, Loss: 0.028266380931457984, Val Loss: 0.027537059632806348\n",
      "Epoch: 136, Loss: 0.028224147273739023, Val Loss: 0.027496402639384097\n",
      "Epoch: 137, Loss: 0.028182353976250085, Val Loss: 0.027456113058088295\n",
      "Epoch: 138, Loss: 0.028140972675814776, Val Loss: 0.02741623257593297\n",
      "Epoch: 139, Loss: 0.028099985087305993, Val Loss: 0.027376707811708745\n",
      "Epoch: 140, Loss: 0.02805938938776522, Val Loss: 0.0273375571538788\n",
      "Epoch: 141, Loss: 0.02801915509848226, Val Loss: 0.027298781127273068\n",
      "Epoch: 142, Loss: 0.027979304180518045, Val Loss: 0.027260445116221506\n",
      "Epoch: 143, Loss: 0.027939860730551325, Val Loss: 0.027222437710759693\n",
      "Epoch: 144, Loss: 0.02790080177259847, Val Loss: 0.027184856484521316\n",
      "Epoch: 145, Loss: 0.027862137106972735, Val Loss: 0.02714765052084816\n",
      "Epoch: 146, Loss: 0.027823859390381737, Val Loss: 0.02711078927147926\n",
      "Epoch: 147, Loss: 0.027785981971229345, Val Loss: 0.02707433843824443\n",
      "Epoch: 148, Loss: 0.027748475394623624, Val Loss: 0.02703828397226033\n",
      "Epoch: 149, Loss: 0.027711319044149887, Val Loss: 0.02700246314468535\n",
      "Epoch: 150, Loss: 0.02767448509423011, Val Loss: 0.02696702181219925\n",
      "Epoch: 151, Loss: 0.027637982027498873, Val Loss: 0.02693184538422453\n",
      "Epoch: 152, Loss: 0.027601786357347897, Val Loss: 0.026897029553812585\n",
      "Epoch: 153, Loss: 0.02756588643249251, Val Loss: 0.0268625378616337\n",
      "Epoch: 154, Loss: 0.027530322387064623, Val Loss: 0.02682837031928269\n",
      "Epoch: 155, Loss: 0.027495076650323448, Val Loss: 0.0267945110660858\n",
      "Epoch: 156, Loss: 0.027460162574743024, Val Loss: 0.026760989855560132\n",
      "Epoch: 157, Loss: 0.027425585344385526, Val Loss: 0.026727808479748573\n",
      "Epoch: 158, Loss: 0.02739133657746244, Val Loss: 0.026694941697081268\n",
      "Epoch: 159, Loss: 0.027357411514887936, Val Loss: 0.026662348100543593\n",
      "Epoch: 160, Loss: 0.027323797365477655, Val Loss: 0.026630041983009407\n",
      "Epoch: 161, Loss: 0.027290468018908876, Val Loss: 0.026598058469825953\n",
      "Epoch: 162, Loss: 0.027257444788210888, Val Loss: 0.026566380895252727\n",
      "Epoch: 163, Loss: 0.027224718949419142, Val Loss: 0.026534966665583998\n",
      "Epoch: 164, Loss: 0.02719225324966796, Val Loss: 0.026503844832365887\n",
      "Epoch: 165, Loss: 0.027160083403771424, Val Loss: 0.026472975005788386\n",
      "Epoch: 166, Loss: 0.02712819100846694, Val Loss: 0.026442397802577882\n",
      "Epoch: 167, Loss: 0.027096586213929415, Val Loss: 0.026412092926139167\n",
      "Epoch: 168, Loss: 0.02706523304398576, Val Loss: 0.02638203454526544\n",
      "Epoch: 169, Loss: 0.027034132123686473, Val Loss: 0.026352243954155027\n",
      "Epoch: 170, Loss: 0.027003287538490323, Val Loss: 0.02632270890088314\n",
      "Epoch: 171, Loss: 0.026972694591817767, Val Loss: 0.026293446916760457\n",
      "Epoch: 172, Loss: 0.02694235198384815, Val Loss: 0.026264373892571167\n",
      "Epoch: 173, Loss: 0.02691223806183089, Val Loss: 0.026235601700677562\n",
      "Epoch: 174, Loss: 0.02688237124543941, Val Loss: 0.026207050099231074\n",
      "Epoch: 175, Loss: 0.02685275852860493, Val Loss: 0.026178752057459634\n",
      "Epoch: 176, Loss: 0.02682337706452673, Val Loss: 0.026150722556663644\n",
      "Epoch: 177, Loss: 0.02679423703978857, Val Loss: 0.026122839649361972\n",
      "Epoch: 178, Loss: 0.026765338989011286, Val Loss: 0.02609523973311792\n",
      "Epoch: 179, Loss: 0.026736659591319777, Val Loss: 0.02606783080118464\n",
      "Epoch: 180, Loss: 0.02670819992633105, Val Loss: 0.0260406655060227\n",
      "Epoch: 181, Loss: 0.026679969635769324, Val Loss: 0.02601372464733724\n",
      "Epoch: 182, Loss: 0.026651951822179085, Val Loss: 0.02598700735627175\n",
      "Epoch: 183, Loss: 0.026624157905508587, Val Loss: 0.02596050313813519\n",
      "Epoch: 184, Loss: 0.02659656735597613, Val Loss: 0.02593419968834197\n",
      "Epoch: 185, Loss: 0.02656917266172512, Val Loss: 0.02590806722321121\n",
      "Epoch: 186, Loss: 0.02654199557101779, Val Loss: 0.02588219097263269\n",
      "Epoch: 187, Loss: 0.026515038888080426, Val Loss: 0.025856494019031444\n",
      "Epoch: 188, Loss: 0.026488288168446675, Val Loss: 0.02583100505372339\n",
      "Epoch: 189, Loss: 0.02646172390136126, Val Loss: 0.025805723602799025\n",
      "Epoch: 190, Loss: 0.026435353101262233, Val Loss: 0.025780608208911246\n",
      "Epoch: 191, Loss: 0.02640916749761886, Val Loss: 0.025755680299900083\n",
      "Epoch: 192, Loss: 0.026383181645831807, Val Loss: 0.02573094333029939\n",
      "Epoch: 193, Loss: 0.026357366536603914, Val Loss: 0.025706363245590315\n",
      "Epoch: 194, Loss: 0.026331722977570454, Val Loss: 0.025681995379454127\n",
      "Epoch: 195, Loss: 0.026306259433988025, Val Loss: 0.02565778985116995\n",
      "Epoch: 196, Loss: 0.026280977085333532, Val Loss: 0.025633782140915627\n",
      "Epoch: 197, Loss: 0.026255885910339474, Val Loss: 0.025609963421240504\n",
      "Epoch: 198, Loss: 0.026230965406202348, Val Loss: 0.025586320663837616\n",
      "Epoch: 199, Loss: 0.026206214417275147, Val Loss: 0.02556285426733125\n",
      "Epoch: 200, Loss: 0.026181630198815538, Val Loss: 0.02553955484164043\n",
      "Epoch: 201, Loss: 0.026157223323782047, Val Loss: 0.025516424390993952\n",
      "Epoch: 202, Loss: 0.026132988888830422, Val Loss: 0.025493472923136362\n",
      "Epoch: 203, Loss: 0.026108922249826482, Val Loss: 0.02547066614650576\n",
      "Epoch: 204, Loss: 0.026085015359504887, Val Loss: 0.025448036325428348\n",
      "Epoch: 205, Loss: 0.02606127298837602, Val Loss: 0.025425566817704384\n",
      "Epoch: 206, Loss: 0.02603767945846634, Val Loss: 0.025403222737888156\n",
      "Epoch: 207, Loss: 0.02601425082219283, Val Loss: 0.02538107392858037\n",
      "Epoch: 208, Loss: 0.025990988492905903, Val Loss: 0.025359058837808205\n",
      "Epoch: 209, Loss: 0.025967882985953296, Val Loss: 0.025337196213399152\n",
      "Epoch: 210, Loss: 0.025944930143692254, Val Loss: 0.02531552311018288\n",
      "Epoch: 211, Loss: 0.025922135424202707, Val Loss: 0.02529400193828094\n",
      "Epoch: 212, Loss: 0.02589949103253103, Val Loss: 0.025272648953804936\n",
      "Epoch: 213, Loss: 0.02587699511722377, Val Loss: 0.025251462362137298\n",
      "Epoch: 214, Loss: 0.025854651576186705, Val Loss: 0.02523042438351942\n",
      "Epoch: 215, Loss: 0.025832449260460095, Val Loss: 0.02520950185333194\n",
      "Epoch: 216, Loss: 0.025810387803804215, Val Loss: 0.025188729828122196\n",
      "Epoch: 217, Loss: 0.025788469967566962, Val Loss: 0.025168076867013004\n",
      "Epoch: 218, Loss: 0.025766689679605635, Val Loss: 0.025147567121693092\n",
      "Epoch: 219, Loss: 0.025745033153382623, Val Loss: 0.025127175525655777\n",
      "Epoch: 220, Loss: 0.025723513973704402, Val Loss: 0.02510693524386907\n",
      "Epoch: 221, Loss: 0.02570213182165798, Val Loss: 0.02508681911980584\n",
      "Epoch: 222, Loss: 0.0256808881925529, Val Loss: 0.025066833001503558\n",
      "Epoch: 223, Loss: 0.025659776090614227, Val Loss: 0.025047021178166807\n",
      "Epoch: 224, Loss: 0.02563880905041472, Val Loss: 0.02502732128822207\n",
      "Epoch: 225, Loss: 0.0256179674093039, Val Loss: 0.025007766835031607\n",
      "Epoch: 226, Loss: 0.0255972442855468, Val Loss: 0.024988288356264775\n",
      "Epoch: 227, Loss: 0.025576634517566435, Val Loss: 0.02496896181214722\n",
      "Epoch: 228, Loss: 0.025556150385006672, Val Loss: 0.02494973041680654\n",
      "Epoch: 229, Loss: 0.025535787569319138, Val Loss: 0.024930630621430912\n",
      "Epoch: 230, Loss: 0.025515553425362582, Val Loss: 0.024911690899012878\n",
      "Epoch: 231, Loss: 0.025495453934932615, Val Loss: 0.024892842809546078\n",
      "Epoch: 232, Loss: 0.025475457750544463, Val Loss: 0.024874140405731476\n",
      "Epoch: 233, Loss: 0.025455581883702606, Val Loss: 0.024855530338246182\n",
      "Epoch: 234, Loss: 0.025435822198006608, Val Loss: 0.02483704534445435\n",
      "Epoch: 235, Loss: 0.0254161633569035, Val Loss: 0.024818677119862992\n",
      "Epoch: 236, Loss: 0.025396615882782244, Val Loss: 0.02480043894010355\n",
      "Epoch: 237, Loss: 0.025377197651751063, Val Loss: 0.024782313973806896\n",
      "Epoch: 238, Loss: 0.025357891577412795, Val Loss: 0.024764303897678166\n",
      "Epoch: 239, Loss: 0.025338708927000485, Val Loss: 0.02474641391593353\n",
      "Epoch: 240, Loss: 0.02531963597952802, Val Loss: 0.024728614883878668\n",
      "Epoch: 241, Loss: 0.0253006626241581, Val Loss: 0.02471094333866697\n",
      "Epoch: 242, Loss: 0.025281783707358577, Val Loss: 0.024693345136909576\n",
      "Epoch: 243, Loss: 0.02526301876432753, Val Loss: 0.0246758856715214\n",
      "Epoch: 244, Loss: 0.02524434850068648, Val Loss: 0.024658511138863436\n",
      "Epoch: 245, Loss: 0.02522577392144586, Val Loss: 0.02464122910081542\n",
      "Epoch: 246, Loss: 0.025207307152371294, Val Loss: 0.024624061312517265\n",
      "Epoch: 247, Loss: 0.025188932868036286, Val Loss: 0.024606979751662258\n",
      "Epoch: 248, Loss: 0.025170669553640425, Val Loss: 0.024590028946716566\n",
      "Epoch: 249, Loss: 0.025152520126971663, Val Loss: 0.024573171940576403\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "# This is where the class is used to train the model\n",
    "\n",
    "# The parameters in the model are (input_neurons, hidden_neurons, output_neurons, learning_rate, epochs)\n",
    "# These parameters aren't the right parameters, so tweak them to get the best results\n",
    "# Around 70% accuracy is a good end goal (75% is great) but for the recruitment task, 60% is good enough\n",
    "\n",
    "# [Q17] What are the parameters in the model and what do they mean?\n",
    "\n",
    "fashion_mnist = NN(784, 128, 10, 0.1, 250)\n",
    "p = np.random.permutation(len(X))\n",
    "X, y = X[p], y[p]\n",
    "\n",
    "# Splitting the data into training, validation and testing in the ratio 70:20:10\n",
    "X_train, y_train = X[:int(0.7*len(X))], y[:int(0.7*len(X))]\n",
    "X_val, y_val = X[int(0.7*len(X)):int(0.9*len(X))], y[int(0.7*len(X)):int(0.9*len(X))]\n",
    "X_test, y_test = X[int(0.9*len(X)):], y[int(0.9*len(X)):]\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "print(f\"Test samples: {len(X_test)}\")\n",
    "\n",
    "# Training the model\n",
    "print(\"\\nTraining model...\")\n",
    "train_loss,val_loss = fashion_mnist.fit(X_train, y_train,X_val,y_val)\n",
    "\n",
    "\n",
    "# Plotting the loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "y_pred = fashion_mnist.predict(X_test)\n",
    "\n",
    "# [Q18] Why are we using argmax here? Why is this output different from the output of the model?\n",
    "y_pred = np.argmax(y_pred, axis=1)  \n",
    "y_test_label = np.argmax(y_test, axis=1)\n",
    "print(f\"Accuracy: {np.mean(y_pred == y_test_label)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {np.mean(y_pred == y_test_label)}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
